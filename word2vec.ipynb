{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Models for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Word2Vec model is a simple word embedding neural network, developed by Mikolov et al. (2013).\n",
    "####  *Such continuous word embedding representations have have been proven to be able to carry semantic meanings and are useful in various NLP tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have attempted to implement three language models described in *Le & Mikolov (2014)*'s paper ** *Distributed Representations of Sentences and Documents* **.\n",
    "\n",
    "The implementations don't make use of any NLP libraries and consist of the simplest form of the algorithm with little optimization.\n",
    "The aim of this notebook is simply to gain:\n",
    "* understanding of the **language models' algorithm**\n",
    "* intuition on **word embedding representations**\n",
    "* understanding of inner workings of **neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, popular fixed-length features used in language models are **bag-of-words**.\n",
    "\n",
    "However bag-of-words features have two major weaknesses: \n",
    "- they lose the **ordering** of the words.\n",
    "- they also ignore **semantics** of the words. \n",
    "\n",
    "In the paper by **Le & Mikolov (2014)**, they propose a distributed representation of sentences and documents, which they call ** *Paragraph Vector* **. \n",
    "\n",
    "The model assumes the ** *Distributional Hypothesis* ** that words are characterized by other words in their vicinity. This idea is used to estimate the probability of two words occurring near each other.\n",
    "\n",
    "*Paragraph Vector* is an **unsupervised algorithm** that learns fixed-length feature representations from **variable-length pieces of texts**, such as sentences and paragraphs. The algorithm represents each word and sentence by a dense vector which is trained to predict words in the document. Its construction gives the algorithm the potential to overcome the **weaknesses** of bag-of- words models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/3.png\"  width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical results, in the paper, show that **Paragraph Vectors outperform bag-of-words models**, as well as, other techniques for text representations. For example, **word weighting functions** (which requires task-specific tuning) and **parse trees**. The authors were able to achieve new **state-of-the-art** results on several **text classification** and **sentiment analysis** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is made possible because words with **similar meaning** are mapped to a **similar position** in the **vector space**. The difference between word vectors also carry meaning. For example, the word vectors can be used to answer analogy questions using simple vector algebra: “King” - “man” + “woman” = “Queen”.\n",
    "Theses properties of word vectors are useful in many NLP task, such as **language modeling and understanding** and **machine translation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Word Context Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this  first language model, we will look at the simplest form of the *Continuous Bag-Of-Word Model (CBOW)* introduced in *Mikolov et al. (2013a)*. This model has just **one word** as the context to predict the next target word in the sentence. This context vector is fed into a **neural network** with a **single hidden layer** that is fully connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/1neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input vector is a **one-hot encoded vector**. Each word is mapped into its own **unique vector of features**, represented by a column in the matrix $W_I$. There is also another weight matrix, $W_O$ connecting the hidden layer to the output layer.\n",
    "\n",
    "Given a **context word** and these **weight matrices**, we can compute a **score** for each word in the vocabulary. This is essentially a **multiclass classification** where we have as many labels as our vocabulary size $V$. **Softmax regression**, a log-linear classification model, is used to compute the **posterior probability $P(W_O|W_I)$:**\n",
    "\n",
    "$$ P(W_O|W_I) = y_i = \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} $$\n",
    "\n",
    "In the paper, **hierarchical softmax** is used for faster training, but for simpliticy I will just use a regular softmax regression as the multiclass classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous embeddings of the words represent **points in a vector space**. What the Word2Vec model is trying to do is assign each word a **meaningful point** in that space that represent its **semantic** in **relation** to the other words. The words are first **initialized to a random location** in that space, the model will then try  to learn better vector positions. Similar words will be **pushed together**, while different ones will be **pulled away.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden-to-output layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learns by maximzing the posterior probability. For **numerical stability**, we will instead **minimize the log loss function**: \n",
    "\n",
    "$$E = -\\log P(W_O|W_I)$$\n",
    "\n",
    "$$E = -\\log \\frac{exp(W_I \\cdot W'^T_O)}{\\sum^V_{j=1} exp(W_I \\cdot W'^T_j)} =-\\log \\frac{exp(u)}{\\sum^V_{j=1} exp(u_j)} $$\n",
    "\n",
    "$$E = -u + \\log \\sum^V_{j=1} exp(u_j)$$\n",
    "\n",
    "The update equation of the weights between **hidden and output layers**:\n",
    "\n",
    "$$\\dfrac{\\partial E}{\\partial u} = -t_j + \\frac{exp(u)}{\\sum^V_{j=1} exp(u_j)}$$\n",
    "$$ \\dfrac{\\partial E}{\\partial u} = P(W_O|W_I) - t_j = e_j$$\n",
    "\n",
    "Where $t_j = 1$ if the **output word is the actual target**, otherwise $t_j = 0$. This derivative is also the **prediction error of the output layer**, $e_j$.\n",
    "\n",
    "The **gradient** on the hidden-to-output weights is:\n",
    "$$ \\dfrac{\\partial E}{\\partial W_O} = \\dfrac{\\partial E}{\\partial u_j} \\cdot \\dfrac{\\partial u_j}{\\partial W_O} = e_j \\cdot h_i$$\n",
    "\n",
    "Where $h_i$ is a copy of the vector corresponding to the **input word**. \n",
    "\n",
    "To **update the  output weights**, $W_O$, for the hidden-to-output layer, **stochastic gradient descent** is used with a learning rate $\\alpha$: \n",
    "\n",
    "$${W_{O}}^{T (new)}_j = {W_{O}}^{T (old)}_j - \\alpha \\cdot e_j \\cdot h_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the input-to-hidden layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **prediction errors** need to be **backpropagated** to the input-to-hidden weights. \n",
    "\n",
    "The derivative of $E$ on the output of the hidden layer is:\n",
    "\n",
    "$$ \\dfrac{\\partial E}{\\partial h_i} = \\sum^V_{j=1} \\dfrac{\\partial E}{\\partial u_j} \\cdot \\dfrac{\\partial u_j}{\\partial h_i} = \\sum^V_{j=1} e_j \\cdot W_O = EH$$\n",
    "\n",
    "EH is the **sum of the hidden-to-output vectors** for each word in the vocabulary **weighted** by their **prediction error**.\n",
    "\n",
    "The **gradient** on the input weights is:\n",
    "\n",
    "$$ \\dfrac{\\partial E}{\\partial W_I} = \\dfrac{\\partial E}{\\partial h_i} \\cdot \\dfrac{\\partial h_i}{\\partial W_I} = EH_i \\cdot x$$\n",
    "\n",
    "Where $x$ is one-hot encoded vector.\n",
    "\n",
    "To **update the  intput weights** for the input-to-hidden layer, we make use again of **stochastic gradient descent**, with a learning rate $\\alpha$: \n",
    "\n",
    "$${W_{I}}^{T (new)}_j = {W_{I}}^{T (old)}_j - \\alpha \\cdot EH $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good idea how the model works, we'd like to see it in action. To keep this notebook **concise and readable** I have kept the code in separate files located in the same directory of this notebook.\n",
    "\n",
    "This **first model** can be found in file * \"Word2Vec_1WordContext.py\" *\n",
    "\n",
    "Let's test this model with a *tiny* toy dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['<s> the prince loves skateboarding in the park </s>', \n",
    "             '<s> the princess loves the prince but the princess hates skateboarding </s>',\n",
    "             '<s> skateboarding in the park is popular </s>',\n",
    "             '<s> the prince is popular but the prince hates attention </s>',\n",
    "             '<s> the princess loves attention but the princess hates the park </s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not expect to learn much with such a small sample. Furthermore, most **neural networks** perform very poorly on small datasets. However, the goal is to test if the model was implemented correctly and gain some **intuition** on how word vectors representation works. \n",
    "\n",
    "Tokens $<s>$  and $</s>$ have been added at the beginning and end of sentences respectively. This conveniently allows the context window to loop over every word **equally** (more on that later).\n",
    "\n",
    "Let's import the file and **instantiate the model** with our tiny dataset with **three nodes** in the hidden layer and learning rate of 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Word2Vec_1WordContext import Word2Vec_1WordContext\n",
    "\n",
    "model1 = Word2Vec_1WordContext(sentences, learning_rate = 1.2, nodes_HL = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is kept simple with only **three hidden nodes** because with such little data not much can be learned. In fact, we are actually better off constraining the model to **low dimensionality** in that case. In addition, three nodes will allow us to plot our word vectors on a **3D graph** and **visualize the neural network features**.\n",
    "\n",
    "The training process has been simplified to a **single iteration for every word in the text**. The function returns the learned **input weight matrix** containing the **word vectors**, and a dictionary of the **text's vocabulary** where the values are the corresponding **row indices** of the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :\n",
      "('<s>', 0)\n",
      "('the', 1)\n",
      "('prince', 2)\n",
      "('loves', 3)\n",
      "('skateboarding', 4)\n",
      "('in', 5)\n",
      "('park', 6)\n",
      "('</s>', 7)\n",
      "('princess', 8)\n",
      "('but', 9)\n",
      "('hates', 10)\n",
      "('is', 11)\n",
      "('popular', 12)\n",
      "('attention', 13)\n",
      "\n",
      " Learned Word Vectors: \n",
      " [[-2.08270488  2.55925557 -2.12930501]\n",
      " [-6.60210368  8.58476006 -6.80716901]\n",
      " [-1.95048465  2.54917797 -1.94794019]\n",
      " [-1.90633806  2.23921986 -1.77413093]\n",
      " [-0.62354432  0.42858976 -0.31709678]\n",
      " [-0.17637633  0.21253038 -0.08490952]\n",
      " [-1.80168588  2.38031994 -1.66203477]\n",
      " [-0.10635291 -0.09666614  0.13161846]\n",
      " [-3.29223587  4.05530739 -3.08323329]\n",
      " [-2.05342986  2.44019742 -1.84177854]\n",
      " [-2.74530684  3.82105132 -2.88493793]\n",
      " [-0.40716979  0.79752959 -0.37286327]\n",
      " [-0.65052351  0.69169371 -0.3817471 ]\n",
      " [-2.86758368  3.64153825 -2.80406576]]\n"
     ]
    }
   ],
   "source": [
    "W, vocab = model1.train()\n",
    "\n",
    "print( \"Vocabulary :\")\n",
    "for kv in vocab.items():\n",
    "    print( kv)\n",
    "print( \"\\n Learned Word Vectors: \\n\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It **worked!** You maybe trust me, but you haven't gained any **intuition** on what has happened. That's the **problem** with **neural nets** they are **hard to interpret**. Fortunately, we have a very **simple model** with vectors constrained to just **three features**. Therefore, we can **visualize** them using the graphing function I incorporated in the class model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'plotly.graph_objs' has no attribute 'sign_in'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_objs \u001b[38;5;28;01mas\u001b[39;00m py\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Add your own Plotly Username and api_key\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign_in\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarvinBertin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1d708sl040\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/plotly/graph_objs/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(import_name)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m()\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_getattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimport_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/_plotly_utils/importers.py:39\u001b[0m, in \u001b[0;36mrelative_import.<locals>.__getattr__\u001b[0;34m(import_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m     class_module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(rel_module, parent_name)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(class_module, class_name)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{__name__!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{name!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     41\u001b[0m         name\u001b[38;5;241m=\u001b[39mimport_name, \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m=\u001b[39mparent_name\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'plotly.graph_objs' has no attribute 'sign_in'"
     ]
    }
   ],
   "source": [
    "from plotly import graph_objs as py\n",
    "# Add your own Plotly Username and api_key\n",
    "py.sign_in('MarvinBertin', '1d708sl040')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was train with very **little data** over a **small number of iterations**, therefore based on how the vectors where **initionalized** the **3D scatter plot** will **differ** a little every time you run it.\n",
    "\n",
    "Nonetheless, there is a **lot to notice** in the **representation** created by the **neural network**. Firstly, we can observe that the words are definitely **not randomly positioned**. The **word vectors** have been **moved** by the algorithm into so kind of a **structure**. \n",
    "\n",
    "For example, the tokens *\"the\"* and $</s>$ are found at both **opposite extremes** of the plot. The fact that *\"the\"* was often found at the beginning of a sentence and $</s>$ at the end, may explain this **relationship**.\n",
    "\n",
    "Furthermore, words like *\"skateboarding\"*, *\"popular\"* and *\"prince\"* are **clustered together**, whereas *\"princess\"*, *\"hates\"* and *\"but\"* form **another cluster**. From different angles, we can observe **additional structures** like *\"is popular\"* and *\"loves attention\"*.\n",
    "\n",
    "These are already pretty **impressive results**, considering how **little data** the model had. It was able to some extent capture some of the **semantics of the text**.\n",
    "\n",
    "Let's see if we can do even **better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Word Context Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is an extention of the *Continuous Bag-Of_Word Model (CBOW)*. The context is now any **number of words**\n",
    "before the **target word** we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/2neural-network-cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent this **context of words**, we simply take the **average of the word vectors** in the context. The hidden layer is now the product of this **new vector** and the **input weight matrix**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h=\\frac{1}{C}W_{I}(x_1+x_2+…+x_C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update for the output weights for the hidden-to-output layer is the same as before:\n",
    "\n",
    "$${W_{O}}^{T (new)}_j = {W_{O}}^{T (old)}_j - \\alpha \\cdot e_j \\cdot h_j$$\n",
    "\n",
    "The update for the input weights is almost identical as well. The only difference is that it now needs to be applied to **every word in the context window**.\n",
    "\n",
    "$${W_{I,c}}^{T (new)}_j = {W_{I,c}}^{T (old)}_j - \\frac{1}{C} \\cdot \\alpha \\cdot EH $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this **second model** can be found in file \"Word2Vec_nWordContext.py\"\n",
    "\n",
    "The learning algorithm is almost identical to the previous model. However, the challenge is now to find a way to allow the **context window to shrink and expand** as it loops over sentences.\n",
    "\n",
    "How do you deal with **any size context window** across **any sentence size**?\n",
    "To illustrate my point, I've created a toy function that helps to visualize how does the **context window moves across a sentence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from context_window import context_window_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "context_window_search(context_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, when the **window is small** and the **sentence long** this is straight forward. The **window slides** at every iteration. However when you have a **large window** and a **small sentence**, you will run in the problem of giving **less importance** to words at the **beginning and end** of the sentence.\n",
    "\n",
    "To get around that you need to **dynamically** change the **size of the window** as it slides across the sentence. The function below does this automatically. \n",
    "\n",
    "It **expands** gradually to the requested window size and then **contracts** as it reaches the end of the sentence. In this way, every word is passed as input excatly **5 times** for a context size of 5 for example. You can check it for yourself below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_window_search(context_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This little change allows for **large improvements** in the model. The neural network can now **learning and relate** a word to both its **adjacent** and **distant neighbors**, allowing for **better semantic understanding**.\n",
    "\n",
    "Let's train this new model on our tiny dataset with this time a **context window of three words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Word2Vec_nWordContext import Word2Vec_nWordContext\n",
    "model2 = Word2Vec_nWordContext(sentences, learning_rate = 1.1, context_size = 3, nodes_HL = 3)\n",
    "W, vocab = model2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the model **pushed the word vectors** into a **structured vector space**. Some of the same previous strucutres are found here. We can also notice, if viewed the space from **different angles**, that more **subtle relationships** are starting to appear. \n",
    "\n",
    "For example, *\"princess\" - \"prince\"* **approximate** to *\"hates\" - \"loves\"*. In some sense, the model was able to capture **relations** between words that can be mapped into **mathematical operations**.\n",
    "\n",
    "To be fair, this is not completely obvious from the plot, and some of it may very well just be due to randomness in this case. However as you increase the **data size**, the **search space dimentionality** and **training time**, this model will be able to extract **deeper meaning and semantinc** from a text **efficently**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Vector Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Paragraph Vector model is another variation of the previous models. In addition to **mapping every word** into **unique vectors**, the model will also **map the paragraphs or sentences** in the text into a **unique vector**. Each represented by a column in matrix $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/3doc2vec.png\"  width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only real difference in the algorithm is with the **computation of the hidden layer**, $h$. Now both the **sentence vector and word vectors** are **averaged** to predict the next word in a context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h=\\frac{1}{C}W_{I}(D_d+x_1+x_2+…+x_C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the **sentence token** as just another **vocabulary word**. However, this token vector tries to represent the **semantics of the entire sentence**. In other words, it acts as a **memory that remembers what is missing from the current context**. For this reason, this model is also referred as the ** *Distributed Memory Model of Paragraph Vectors (PV-DM)* **.\n",
    "\n",
    "As the context window travels across a sentence, the **sentence vector is unique**, whereas the **context vector changes** constantly. After being trained, the paragraph vectors can be used as **features** for the paragraph instead of **additional bag-of-words**. These features can then be used directly in other **machine learning models**, such as logistic regression or K-means.\n",
    "\n",
    "This allows the model to perform **better than a bag-of-n-grams model** because it is able to **generalize the text features** without **suffering from very high-dimensional representations** like in a bag of n-grams model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of this **third language model** was fairly straight forward, but **concenptually** the **extra paragraph token** added has the potential to greatly boost its **learning capabilities**.\n",
    "\n",
    "Let's train on our tiny dataset one last time and see the difference this model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Doc2Vec_nWordContext import Doc2Vec_nWordContext\n",
    "model3 = Doc2Vec_nWordContext(sentences, learning_rate = 0.8, context_size = 3, nodes_HL = 3)\n",
    "W, D, vocab = model3.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function now also returns the $D$ weight matrix, which is the **learned representation of each five sentences in the text**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model3.graph_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The plot of the learned word vectors, now also include the **sentence representations** (S1, S2, etc.). \n",
    "\n",
    "The first thing we can notice is that the sentence vectors have been **physically separated** from the word vectors. The model was able to **differentiate** between **individual words** and **entire sections of the text**. \n",
    "\n",
    "Furthermore, sentence vectors are grouped into a **strucutre of their own**. **S1 and S3** are together while **S2 and S4** are in another region. The first group of sentences are about **skateboarding in the park**, while the other cluster have sentences with very **similar grammatical structure**. They **compare** what the prince and princess love and hate.\n",
    "\n",
    "As discussed earlier the Paragraph Vector model takes a **general approach** and is capable of having some sort of **memory**. Therefore it can divide a text into **topics** in an **un-surpervised** way.\n",
    "\n",
    "This is **powerful** idea which allows to learn a text at **different levels or scales** (words, sentences, paragraphs). In fact, this is similar to **convolution nets** that use **max pooling** to **zoom out** of an image and learning different **feature abstractions** embedded in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scale up to real datasets\n",
    "* Learn about NLP libraries like gensim\n",
    "* Move to more sophisticated models (hierarchical softmax, negative sampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
